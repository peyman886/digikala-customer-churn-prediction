{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Model Training for Churn Prediction\n",
    "\n",
    "Train and evaluate multiple ML models to predict customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, classification_report, \n",
    "    confusion_matrix, precision_recall_curve, f1_score\n",
    ")\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "print('‚úÖ Libraries imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load engineered features\n",
    "df = pd.read_csv('../data/user_features.csv')\n",
    "\n",
    "print(f'üìä Dataset shape: {df.shape}')\n",
    "print(f'üéØ Churn rate: {df[\"is_churned\"].mean():.2%}')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(['user_id', 'is_churned'], axis=1)\n",
    "y = df['is_churned']\n",
    "\n",
    "print(f'üìä Features shape: {X.shape}')\n",
    "print(f'üéØ Target shape: {y.shape}')\n",
    "print(f'\\nüìã Feature names:')\n",
    "print(list(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (stratified to preserve class distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'üìä Train set: {X_train.shape}')\n",
    "print(f'üìä Test set: {X_test.shape}')\n",
    "print(f'\\nüéØ Class distribution:')\n",
    "print(f'  Train churn rate: {y_train.mean():.2%}')\n",
    "print(f'  Test churn rate: {y_test.mean():.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (for Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('‚úÖ Features scaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ Model 1: Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr_model.predict(X_test_scaled)\n",
    "lr_pred_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "lr_auc = roc_auc_score(y_test, lr_pred_proba)\n",
    "lr_f1 = f1_score(y_test, lr_pred)\n",
    "\n",
    "print(f'üìä Logistic Regression Results:')\n",
    "print(f'   ROC-AUC: {lr_auc:.4f}')\n",
    "print(f'   F1-Score: {lr_f1:.4f}')\n",
    "print(f'\\n{classification_report(y_test, lr_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "rf_auc = roc_auc_score(y_test, rf_pred_proba)\n",
    "rf_f1 = f1_score(y_test, rf_pred)\n",
    "\n",
    "print(f'üìä Random Forest Results:')\n",
    "print(f'   ROC-AUC: {rf_auc:.4f}')\n",
    "print(f'   F1-Score: {rf_f1:.4f}')\n",
    "print(f'\\n{classification_report(y_test, rf_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ Model 3: XGBoost (Best Expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scale_pos_weight for imbalance\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f'‚öñÔ∏è Scale pos weight: {scale_pos_weight:.2f}')\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "xgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "xgb_auc = roc_auc_score(y_test, xgb_pred_proba)\n",
    "xgb_f1 = f1_score(y_test, xgb_pred)\n",
    "\n",
    "print(f'\\nüìä XGBoost Results:')\n",
    "print(f'   ROC-AUC: {xgb_auc:.4f}')\n",
    "print(f'   F1-Score: {xgb_f1:.4f}')\n",
    "print(f'\\n{classification_report(y_test, xgb_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],\n",
    "    'ROC-AUC': [lr_auc, rf_auc, xgb_auc],\n",
    "    'F1-Score': [lr_f1, rf_f1, xgb_f1]\n",
    "}).sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print('üìä Model Comparison:')\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Best model\n",
    "best_model_name = results.iloc[0]['Model']\n",
    "print(f'\\nüèÜ Best Model: {best_model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: ROC Curves\n",
    "for name, y_proba in [('LR', lr_pred_proba), ('RF', rf_pred_proba), ('XGB', xgb_pred_proba)]:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    axes[0].plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})', linewidth=2)\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curves Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Confusion Matrix (XGBoost)\n",
    "cm = confusion_matrix(y_test, xgb_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix (XGBoost)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print('‚úÖ Saved: reports/model_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from XGBoost\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print('üîù Top 10 Most Important Features:')\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = feature_importance.head(10)\n",
    "plt.barh(top_features['feature'], top_features['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 10 Feature Importances (XGBoost)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "print('\\n‚úÖ Saved: reports/feature_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Save Model & Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model (XGBoost)\n",
    "joblib.dump(xgb_model, '../app/model.pkl')\n",
    "print('‚úÖ Saved: app/model.pkl')\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, '../app/scaler.pkl')\n",
    "print('‚úÖ Saved: app/scaler.pkl')\n",
    "\n",
    "# Save feature names\n",
    "with open('../app/feature_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(X.columns))\n",
    "print('‚úÖ Saved: app/feature_names.txt')\n",
    "\n",
    "# Save model performance\n",
    "results.to_csv('../reports/model_performance.csv', index=False)\n",
    "print('‚úÖ Saved: reports/model_performance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "Model training completed successfully!\n",
    "\n",
    "### üéØ Key Results:\n",
    "- ‚úÖ Trained 3 models: LR, RF, XGBoost\n",
    "- ‚úÖ XGBoost achieved best performance\n",
    "- ‚úÖ Handled class imbalance with scale_pos_weight\n",
    "- ‚úÖ Saved model artifacts for deployment\n",
    "\n",
    "### üìà Top 5 Churn Predictors:\n",
    "1. days_since_last_order\n",
    "2. avg_order_frequency\n",
    "3. on_time_ratio\n",
    "4. total_complaints\n",
    "5. avg_sentiment\n",
    "\n",
    "**Next Step: Deploy API with FastAPI**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}